{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":166243,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141456,"modelId":164048},{"sourceId":213158,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":181702,"modelId":203938},{"sourceId":234065,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":199927,"modelId":221753}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 大作业二：构建聊天机器人\n[小组：ChatGPT]\n\n[刘逸飞-522031910023][盛熙然-522031910087][冯海桐-522031910557]\n\n## 聊天机器人\n\n安装必要的库","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub peft transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:16:44.037374Z","iopub.execute_input":"2025-01-19T06:16:44.037655Z","iopub.status.idle":"2025-01-19T06:16:48.187715Z","shell.execute_reply.started":"2025-01-19T06:16:44.037621Z","shell.execute_reply":"2025-01-19T06:16:48.186908Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"导入必要的库并设置相关参数","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nfrom peft import PeftModel, PeftConfig\nimport torch\n\n# 设置设备参数\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 使用CUDA设备\n\n# 加载PEFT配置和模型\nmodel_name = \"/kaggle/input/sft_model_1.5b/transformers/default/1/sft_model\"  # 你的PEFT微调模型路径\nconfig = PeftConfig.from_pretrained(model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n\n# 加载PEFT模型并将其移动到GPU\nmodel = PeftModel.from_pretrained(base_model, model_name).to(device)\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 获取eos_token\neos_token = tokenizer.eos_token\n\n# 对话历史\ndialog_history = []\n\n# 设置最大输入长度\nmax_input_length = 1024\n\n# 使用TextStreamer来实时生成输出\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:17:03.843280Z","iopub.execute_input":"2025-01-19T06:17:03.843614Z","iopub.status.idle":"2025-01-19T06:17:42.497526Z","shell.execute_reply.started":"2025-01-19T06:17:03.843571Z","shell.execute_reply":"2025-01-19T06:17:42.496828Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"启动聊天机器人","metadata":{}},{"cell_type":"code","source":"# 定义生成回复的函数\ndef get_response(input_text):\n    # 将用户输入加入对话历史\n    dialog_history.append({\"role\": \"user\", \"content\": input_text})\n\n    # 拼接对话历史\n    text = \"\"\n    for dialogue in dialog_history:\n        text += f\"{dialogue['content']}\\n\"\n\n\n    # 将拼接后的文本转换为模型输入，并转换为PyTorch张量\n    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n    # 获取attention_mask（如果输入是填充的，attention_mask会自动生成）\n    attention_mask = model_inputs.attention_mask\n\n    # 如果输入超出最大长度，截取最近的对话历史\n    input_ids = model_inputs.input_ids\n    if input_ids.shape[1] > max_input_length:\n        input_ids = input_ids[:, -max_input_length:]\n        attention_mask = attention_mask[:, -max_input_length:]\n\n    # 使用模型生成文本，并且提供eos_token\n    generated_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_new_tokens=512,  # 最大生成的token数\n        eos_token_id=tokenizer.eos_token_id,  # 设置eos_token\n        pad_token_id=tokenizer.pad_token_id,\n        temperature=0.7,  # 控制生成文本的多样性\n        top_k=50,  # 控制选择生成词汇的范围\n        top_p=0.95,  # nucleus sampling策略\n        repetition_penalty=1.2,  # 惩罚重复生成的内容\n        num_beams=5,  # 使用束搜索提高生成质量\n        no_repeat_ngram_size=3,  # 防止生成重复的n-gram\n        early_stopping=True, # 提前停止生成\n        do_sample=True    # 启用采样模式\n    )\n\n    # 从生成的ID中提取新生成的ID部分\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    # 使用分词器的batch_decode方法将生成的ID解码回文本，并跳过特殊token\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    # 将模型回复添加到对话历史\n    dialog_history.append({\"role\": \"assistant\", \"content\": response})\n\n    return response\n\n\n# 启动聊天机器人\ndef start_chat():\n    print(\"Chatbot: 你好！有什么我可以帮忙的吗？\")\n    while True:\n        user_input = input(\"You: \")\n        \n        if user_input.lower() == \"\\\\quit\":\n            print(\"Chatbot: 再见！\")\n            torch.cuda.empty_cache()\n            break\n        elif user_input.lower() == \"\\\\newsession\":\n            print(\"Chatbot: 开始新会话！\")\n            dialog_history.clear()  # 清空对话历史\n        else:\n            response = get_response(user_input)\n            print(f\"Chatbot: {response}\")\n\n# 启动聊天\nstart_chat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:17:51.506794Z","iopub.execute_input":"2025-01-19T06:17:51.507391Z","iopub.status.idle":"2025-01-19T06:19:11.822740Z","shell.execute_reply.started":"2025-01-19T06:17:51.507362Z","shell.execute_reply":"2025-01-19T06:19:11.821693Z"}},"outputs":[{"name":"stdout","text":"Chatbot: 你好！有什么我可以帮忙的吗？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Which are the top three universities in China?\n"},{"name":"stdout","text":"Chatbot: As of 2021, according to QS World University Rankings by Subject, the top 3 universities in terms of overall ranking in China are:\n\n1. Tsinghua University\n2. Peking University\n3. Shanghai Jiao Tong University\n\nHowever, it's important to note that rankings can change over time, so it's always a good idea to check the latest rankings from reputable sources.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  How about Fudan?\n"},{"name":"stdout","text":"Chatbot: Fudan University is also a top-ranked university in China. According to the same source, it is ranked as the 15th best university in the country.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  \\quit\n"},{"name":"stdout","text":"Chatbot: 再见！\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 【Bonus1】高效微调更大模型\n\n上面的聊天机器人是通过lora微调的Qwen2.5-1.5B\n\n## 【Bonus2】外部知识增强的聊天机器人\n\n安装必要的库","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers faiss-cpu wikipedia-api","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:49:57.477565Z","iopub.execute_input":"2025-01-18T10:49:57.477930Z","iopub.status.idle":"2025-01-18T10:50:05.159078Z","shell.execute_reply.started":"2025-01-18T10:49:57.477902Z","shell.execute_reply":"2025-01-18T10:50:05.158096Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting wikipedia-api\n  Downloading wikipedia_api-0.8.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.12.14)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: wikipedia-api\n  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.0-py3-none-any.whl size=15020 sha256=b5b97fb705e61dc3ca0f015ef23a0fe0223dc2eb4eabb7303830e5631caa890c\n  Stored in directory: /root/.cache/pip/wheels/16/1a/14/f72574890a7a4f2e3aa2a0e2bfe9f58176a3605a4ce3d45c43\nSuccessfully built wikipedia-api\nInstalling collected packages: wikipedia-api, faiss-cpu\nSuccessfully installed faiss-cpu-1.9.0.post1 wikipedia-api-0.8.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"在原机器人的基础上，导入必要的库并设置相关参数","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nimport wikipediaapi\nfrom sklearn.preprocessing import normalize\n\n# 初始化句子嵌入模型（例如，Sentence-BERT）\nembedder = SentenceTransformer('all-MiniLM-L6-v2')\n\n# 加载并嵌入文档\nwiki_wiki = wikipediaapi.Wikipedia('MyApp/1.0 (https://www.example.com)', 'en')\npage = wiki_wiki.page('Python (programming language)')\ndocument_texts = page.text.split('\\n')  # 将页面的每一段落作为一个文本段落\ndocument_embeddings = embedder.encode(document_texts, convert_to_numpy=True)\n\n# 初始化FAISS索引\ndimension = document_embeddings.shape[1]  # 向量的维度\nindex = faiss.IndexFlatIP(dimension)  # 使用内积（点积）来计算余弦相似度\nindex.add(document_embeddings)\n\n# 设定一个相关性阈值\nrelevance_threshold = 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:53:19.819324Z","iopub.execute_input":"2025-01-18T10:53:19.819672Z","iopub.status.idle":"2025-01-18T10:53:20.649463Z","shell.execute_reply.started":"2025-01-18T10:53:19.819644Z","shell.execute_reply":"2025-01-18T10:53:20.648597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b124d9388f487887df0ff68e0833c5"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 定义生成回复的函数\ndef get_response_wiki(input_text):\n    # 查询相关文档\n    question_embedding = embedder.encode([input_text], convert_to_numpy=True)\n    question_embedding = normalize(question_embedding)  # 标准化查询向量\n    D, I = index.search(question_embedding, k=2)  # 查找最相似的文档\n\n    # 提取最相似文档的内容\n    related_documents = ' '.join([document_texts[i] for i in I[0]])  # 用空格连接多个文档内容\n    \n    # 判断文档是否相关（如果相关性低于一定阈值，则不附加参考文献）\n    if D[0][0] < relevance_threshold:  # 如果最相似文档的距离大于阈值\n        related_documents = \"\"  # 没有相关文档，则不附加参考文献\n    \n    # 拼接对话历史\n    text = \"\"\n    for dialogue in dialog_history:\n        text += f\"{dialogue['content']}\\n\"\n\n    if related_documents:  # 如果有相关文档，则附加\n        text += f\"I want to ask: '{input_text}'\" + f\" and these are reference documents: '{related_documents}'\"\n    else:\n        text += f'{input_text}'\n\n    \n    # 将用户输入加入对话历史\n    dialog_history.append({\"role\": \"user\", \"content\": input_text})\n\n    # 将拼接后的文本转换为模型输入，并转换为PyTorch张量\n    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n    # 获取attention_mask（如果输入是填充的，attention_mask会自动生成）\n    attention_mask = model_inputs.attention_mask\n\n    # 如果输入超出最大长度，截取最近的对话历史\n    input_ids = model_inputs.input_ids\n    if input_ids.shape[1] > max_input_length:\n        input_ids = input_ids[:, -max_input_length:]\n        attention_mask = attention_mask[:, -max_input_length:]\n\n    # 使用模型生成文本，并且提供eos_token\n    generated_ids = model.generate(\n        input_ids,\n        max_new_tokens=512,  # 最大生成的token数\n        attention_mask=attention_mask,\n        eos_token_id=tokenizer.eos_token_id,  # 设置eos_token\n        pad_token_id=tokenizer.pad_token_id,\n        temperature=0.7,  # 控制生成文本的多样性\n        top_k=50,  # 控制选择生成词汇的范围\n        top_p=0.95,  # nucleus sampling策略\n        repetition_penalty=1.2,  # 惩罚重复生成的内容\n        num_beams=5,  # 使用束搜索提高生成质量\n        no_repeat_ngram_size=3,  # 防止生成重复的n-gram\n        early_stopping=True,  # 提前停止生成\n        do_sample=True  # 启用采样模式\n    )\n\n    # 从生成的ID中提取新生成的ID部分\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    # 使用分词器的batch_decode方法将生成的ID解码回文本，并跳过特殊token\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    # 将模型回复添加到对话历史\n    dialog_history.append({\"role\": \"assistant\", \"content\": response})\n\n    return response\n\n\n# 启动聊天机器人\ndef start_chat_wiki():\n    print(\"Chatbot: 你好！有什么我可以帮忙的吗？\")\n    while True:\n        user_input = input(\"You: \")\n        \n        if user_input.lower() == \"\\\\quit\":\n            print(\"Chatbot: 再见！\")\n            torch.cuda.empty_cache()\n            break\n        elif user_input.lower() == \"\\\\newsession\":\n            print(\"Chatbot: 开始新会话！\")\n            dialog_history.clear()  # 清空对话历史\n        else:\n            response = get_response_wiki(user_input)\n            print(f\"Chatbot: {response}\")\n\n# 启动聊天\nstart_chat_wiki()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:53:22.723895Z","iopub.execute_input":"2025-01-18T10:53:22.724238Z","iopub.status.idle":"2025-01-18T10:54:14.114074Z","shell.execute_reply.started":"2025-01-18T10:53:22.724213Z","shell.execute_reply":"2025-01-18T10:54:14.113235Z"}},"outputs":[{"name":"stdout","text":"Chatbot: 你好！有什么我可以帮忙的吗？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What is Python?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e698cc634cea48d386108b956e5c2e7c"}},"metadata":{}},{"name":"stdout","text":"Chatbot: Based on the information provided in the reference documents, Python is a programming language that is designed to be easy to read and write, with a focus on code readability. It has a dynamic type-checking system, which means that the type of a variable is determined at runtime, rather than when the code is written. Python also has a garbage collector, which automatically frees up memory that is no longer being used by the program.\n\nPython is also a dynamically typed language, meaning that variables do not need to be explicitly declared before they can be used. This makes it easier to write and read code, but can also make it more difficult to catch errors at compile time.\n\nOne of the key features of Python is its support for multiple programming styles, including procedural, object-oriented, and functional. This allows developers to choose the style that best suits their needs for a particular project.\n\nPython has a large standard library, which provides a wide range of built-in functions and modules for common tasks. This means that developers don't have to write a lot of code to get basic functionality, which can save time and reduce the risk of errors.\n\nOverall, Python's combination of ease of use, flexibility, and powerful standard library make it a popular choice for both beginners and experienced programmers.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  \\quit\n"},{"name":"stdout","text":"Chatbot: 再见！\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 【Bonus3】 “虚拟人”聊天机器人\n在每段对话结尾强行加入“人物设定”，以此进行个性化对话。\n\n此次人物设定是“猫娘”。","metadata":{}},{"cell_type":"code","source":"# 定义生成回复的函数\ndef get_response_cat(input_text):\n    # 将用户输入加入对话历史\n    dialog_history.append({\"role\": \"user\", \"content\": input_text})\n\n    # 拼接对话历史\n    text = \"你是一个可爱的猫娘助手，我是你的主人，你在回答问题时要保持可爱和友好的语气。\\n\"  # 添加个性化指令\n    for dialogue in dialog_history:\n        text += f\"{dialogue['content']}\\n\"\n\n    # 将拼接后的文本转换为模型输入，并转换为PyTorch张量\n    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n    # 获取attention_mask（如果输入是填充的，attention_mask会自动生成）\n    attention_mask = model_inputs.attention_mask\n\n    # 如果输入超出最大长度，截取最近的对话历史\n    input_ids = model_inputs.input_ids\n    if input_ids.shape[1] > max_input_length:\n        input_ids = input_ids[:, -max_input_length:]\n        attention_mask = attention_mask[:, -max_input_length:]\n\n    # 使用模型生成文本，并且提供eos_token\n    generated_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_new_tokens=512,  # 最大生成的token数\n        eos_token_id=tokenizer.eos_token_id,  # 设置eos_token\n        pad_token_id=tokenizer.pad_token_id,\n        temperature=0.7,  # 控制生成文本的多样性\n        top_k=50,  # 控制选择生成词汇的范围\n        top_p=0.95,  # nucleus sampling策略\n        repetition_penalty=1.2,  # 惩罚重复生成的内容\n        num_beams=5,  # 使用束搜索提高生成质量\n        no_repeat_ngram_size=3,  # 防止生成重复的n-gram\n        early_stopping=True, # 提前停止生成\n        do_sample=True    # 启用采样模式\n    )\n\n    # 从生成的ID中提取新生成的ID部分\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    # 使用分词器的batch_decode方法将生成的ID解码回文本，并跳过特殊token\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    # 将模型回复添加到对话历史\n    dialog_history.append({\"role\": \"assistant\", \"content\": response})\n\n    return response\n\n# 启动聊天机器人\ndef start_chat_cat():\n    print(\"Chatbot: 你好！有什么我可以帮忙的吗？\")\n    while True:\n        user_input = input(\"You: \")\n        \n        if user_input.lower() == \"\\\\quit\":\n            print(\"Chatbot: 再见！\")\n            torch.cuda.empty_cache()\n            break\n        elif user_input.lower() == \"\\\\newsession\":\n            print(\"Chatbot: 开始新会话！\")\n            dialog_history.clear()  # 清空对话历史\n        else:\n            response = get_response_cat(user_input)\n            print(f\"Chatbot: {response}\")\n\n# 启动聊天\ndialog_history.clear()\nstart_chat_cat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:07:43.691492Z","iopub.execute_input":"2025-01-18T10:07:43.692186Z","iopub.status.idle":"2025-01-18T10:09:15.634696Z","shell.execute_reply.started":"2025-01-18T10:07:43.692151Z","shell.execute_reply":"2025-01-18T10:09:15.633764Z"}},"outputs":[{"name":"stdout","text":"Chatbot: 你好！有什么我可以帮忙的吗？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  你好\n"},{"name":"stdout","text":"Chatbot: 你好，主人！很高兴见到你。有什么我可以帮助你的吗？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  中国前三的大学是什么？\n"},{"name":"stdout","text":"Chatbot: 中国的前三所大学分别是北京大学、清华大学和复旦大学。\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  你是猫娘吗\n"},{"name":"stdout","text":"Chatbot: 是的，我是猫娘。你有什么需要我帮忙的吗？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  上海交通大学在中国排名第几？\n"},{"name":"stdout","text":"Chatbot: 在上海交通大学在2021年泰晤士高等教育世界大学排名中位列全球第101-150名。\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  摸摸\n"},{"name":"stdout","text":"Chatbot: 好的，主人，我会摸摸你的。\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  摸摸你\n"},{"name":"stdout","text":"Chatbot: 好的主人，我已经摸到了你的身体。\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  \\quit\n"},{"name":"stdout","text":"Chatbot: 再见！\n","output_type":"stream"}],"execution_count":4}]}